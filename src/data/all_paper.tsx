import LARP_figure from "@/assets/img/paper/LARP.png";
import BundleGT_figure from "@/assets/img/paper/BundleGT.png";
import CLHE_figure from "@/assets/img/paper/CLHE.png";
import ElimRec_figure from "@/assets/img/paper/ElimRec.png";
import SLMRec_figure from "@/assets/img/paper/SLMRec.png";
import BundleLLM_figure from "@/assets/img/paper/BundleLLM.png";
import ModalBed_figure from "@/assets/img/paper/ModalBed.png";
import ToDA_figure from "@/assets/img/paper/ToDA.png";
import DNS_figure from "@/assets/img/paper/DNS.png";
import DyViM_figure from "@/assets/img/paper/DyViM.png";
import LLM2Rec_figure from "@/assets/img/paper/LLM2Rec.png";
import LMTP_figure from "@/assets/img/paper/LMTP.png";
import AlphaSteer_figure from "@/assets/img/paper/AlphaSteer.png";
import PMRL_figure from "@/assets/img/paper/PMRL.png";
import CalMRL_figure from "@/assets/img/paper/CalMRL.png";

export const CalMRL = {
  title:
    "Calibrated Multimodal Representation Learning with Missing Modalities",
  authors: (
    <>
      <strong key="me">Xiaohao Liu</strong>, Xiaobo Xia, Jiaheng Wei, Shuo Yang,
      Xiu Su, See-Kiong Ng, Tat-Seng Chua
    </>
  ),
  accepted: "",
  date: "Nov 2025",
  remark: <></>,
  thumbnail: CalMRL_figure,
  paper_link: "https://arxiv.org/pdf/2511.12034",
  code_link: "",
};

export const PMRL = {
  title: "Principled Multimodal Representation Learning",
  authors: (
    <>
      <strong key="me">Xiaohao Liu</strong>, Xiaobo Xia, See-Kiong Ng, Tat-Seng
      Chua
    </>
  ),
  accepted: "",
  date: "Jul 2025",
  remark: <></>,
  thumbnail: PMRL_figure,
  paper_link: "https://arxiv.org/pdf/2507.17343",
  code_link: "",
};

export const LMTP = {
  title:
    "L-MTP: Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models",
  authors: (
    <>
      <strong key="me">Xiaohao Liu</strong>, Xiaobo Xia, Weixiang Zhao, Manyi
      Zhang, Xianzhi Yu, Xiu Su, Shuo Yang, See-Kiong Ng, Tat-Seng Chua
    </>
  ),
  accepted: "NeurIPS",
  date: "Sep 2025",
  remark: <></>,
  thumbnail: LMTP_figure,
  paper_link: "https://arxiv.org/pdf/2505.17505",
  code_link: "https://github.com/Xiaohao-Liu/L-MTP",
};

export const AlphaSteer = {
  title:
    "AlphaSteer: Learning Refusal Steering with Principled Null-Space Constraint",
  authors: (
    <>
      Leheng Sheng, Changshuo Shen, Weixiang Zhao, Junfeng Fang,{" "}
      <strong key="me">Xiaohao Liu</strong>, Zhenkai Liang, Xiang Wang, An
      Zhang, Tat-Seng Chua
    </>
  ),
  accepted: "",
  date: "May 2025",
  remark: <></>,
  thumbnail: AlphaSteer_figure,
  paper_link: "https://arxiv.org/pdf/2506.07022",
  code_link: "https://github.com/AlphaLab-USTC/AlphaSteer",
};

export const LLM2Rec = {
  title:
    "LLM2Rec: Large Language Models Are Powerful Embedding Models for Sequential Recommendation",
  authors: (
    <>
      Yingzhi He*, <strong key="me">Xiaohao Liu*</strong>, An Zhang, Yunshan Ma,
      Tat-Seng Chua
    </>
  ),
  accepted: "KDD",
  date: "Aug 2025",
  remark: <></>,
  thumbnail: LLM2Rec_figure,
  paper_link: "https://arxiv.org/pdf/2506.21579",
  code_link: "https://github.com/HappyPointer/LLM2Rec",
};

export const DyViM = {
  title: "Extending Visual Dynamics for Video-to-Music Generation",
  authors: (
    <>
      <strong key="me">Xiaohao Liu</strong>, Teng Tu, Yunshan Ma, Tat-Seng Chua
    </>
  ),
  accepted: "",
  date: "Apr 2025",
  remark: <></>,
  thumbnail: DyViM_figure,
  paper_link: "https://arxiv.org/pdf/2504.07594",
  code_link: "",
};

export const DNS = {
  title: "Continual Multimodal Contrastive Learning",
  authors: (
    <>
      <strong key="me">Xiaohao Liu</strong>, Xiaobo Xia, See-Kiong Ng, Tat-Seng
      Chua
    </>
  ),
  accepted: "NeurIPS",
  date: "Sep 2025",
  remark: <></>,
  thumbnail: DNS_figure,
  paper_link: "https://arxiv.org/pdf/2503.14963",
  code_link: "https://github.com/Xiaohao-Liu/CMCL",
};

export const ModalBed = {
  title:
    "Towards Modality Generalization: A Benchmark and Prospective Analysis",
  authors: (
    <>
      <strong key="me">Xiaohao Liu</strong>, Xiaobo Xia, Zhuo Huang, See-Kiong
      Ng, Tat-Seng Chua
    </>
  ),
  accepted: "MM (Oral)",
  date: "Aug 2025",
  remark: <></>,
  thumbnail: ModalBed_figure,
  paper_link: "https://arxiv.org/pdf/2412.18277",
  code_link: "https://github.com/Xiaohao-Liu/ModalBed",
};

export const BundleMLLM = {
  title: "Fine-tuning Multimodal Large Language Models for Product Bundling",
  authors: (
    <>
      <strong key="me">Xiaohao Liu</strong>, Jie Wu, Zhulin Tao, Yunshan Ma,
      Yinwei Wei, Tat-Seng Chua
    </>
  ),
  accepted: "KDD",
  date: "Aug 2025",
  remark: <> </>,
  thumbnail: BundleLLM_figure,
  paper_link: "https://arxiv.org/pdf/2407.11712",
  code_link: "https://github.com/Xiaohao-Liu/Bundle-MLLM",
};
export const LARP = {
  title:
    "LARP: Language Audio Relational Pre-training for Cold-Start Playlist Continuation",
  authors: (
    <>
      Rebecca Salganik*,
      <strong key="me">Xiaohao Liu*</strong>, Yunshan Ma, Jian Kang, Tat-Seng
      Chua
    </>
  ),
  accepted: "KDD",
  date: "Aug 2024",
  remark: (
    <>
      We introduce LARP, a multi-modal cold-start playlist continuation model
      which employs a three-layered contrastive learning framework that
      integrates both multi-modal and relational signals into its learned
      representations with increasing layers of task-specific pdftraction:
      within-track (language-audio) contrastive loss, track-track contrastive
      loss, and track-playlist contrastive loss.
    </>
  ),
  thumbnail: LARP_figure,
  paper_link: "https://arxiv.org/pdf/2406.14333",
  code_link: "https://github.com/Rsalganik1123/LARP",
};
export const CLHE = {
  title:
    "Leveraging Multimodal Features and Item-level User Feedback for Bundle Construction",
  authors: (
    <>
      Yunshan Ma,
      <strong key="me">Xiaohao Liu</strong>, Yinwei Wei, Zhulin Tao, Xiang Wang,
      Tat-Seng Chua,
    </>
  ),
  accepted: "WSDM",
  date: "Oct 2024",
  remark: (
    <>
      We learn effective representations by optimally unifying multiple
      features, while addressing the issues of modality missing, noise, and
      sparsity, thus introducing the Contrastive Learning-enhanced Hierarchical
      Encoder method (CLHE).
    </>
  ),
  thumbnail: CLHE_figure,
  paper_link: "https://arxiv.org/pdf/2310.18770",
  code_link: "https://github.com/Xiaohao-Liu/CLHE",
};

export const BundleGT = {
  title: "Strategy-aware Bundle Recommender System",
  authors: (
    <>
      Yinwei Wei,
      <strong key="me">Xiaohao Liu</strong>, Yunshan Ma, Xiang Wang, Liqiang
      Nie, Tat-Seng Chua,
    </>
  ),
  accepted: "SIGIR",
  date: "Apr 2023",
  remark: (
    <>
      Bundle Graph Transformer, termed BundleGT, is a novel model for bundle
      recommendation, which explore the strategy-aware user and bundle
      representations for user-bundle interaction prediction.
    </>
  ),
  thumbnail: BundleGT_figure,
  paper_link: "https://dl.acm.org/doi/10.1145/3539618.3591771",
  code_link: "https://github.com/Xiaohao-Liu/BundleGT",
};

export const EliMRec = {
  title: "EliMRec: Eliminating Single-modal Bias in Multimedia Recommendation",
  authors: (
    <>
      <strong key="me">Xiaohao Liu</strong>, Zhulin Tao, Jiahong Shao, Lifang
      Yang, Xianglin Huang,
    </>
  ),
  accepted: "MM",
  date: "June 2022",
  remark: (
    <>
      We explored single-modal bias by revealing the inner working of
      multi-modal fusion and achieved a generic framework EliMRec so as to
      eliminate single-modal bias in multimedia recommendation.
    </>
  ),
  thumbnail: ElimRec_figure,
  paper_link: "https://dl.acm.org/doi/pdf/10.1145/3503161.3548404",
  code_link: "https://github.com/Xiaohao-Liu/EliMRec",
};
export const SLMRec = {
  title: "Self-supervised Learning for Multimedia Recommendation",
  authors: (
    <>
      Zhulin Tao,
      <strong key="me">Xiaohao Liu</strong>, Yewei Xia, Xiang Wang, Lifang Yang,
      Xianglin Huang, Tat-Seng Chua,
    </>
  ),
  accepted: "TMM",
  date: "June 2022",
  remark: (
    <>
      We go beyond the supervised learning paradigm, and incorporate the idea of
      self-supervised learning (SSL) into multimedia recommendation, to capture
      multi-modal patterns in the data itself.
    </>
  ),
  thumbnail: SLMRec_figure,
  paper_link: "https://ieeexplore.ieee.org/document/9811387",
  code_link: "https://github.com/Xiaohao-Liu/SLMRec",
};
export const ToDA = {
  title:
    "ToDA: Target-oriented Diffusion Attacker against Recommendation System ",
  authors: (
    <>
      <strong key="me">Xiaohao Liu</strong>, Zhulin Tao, Ting Jiang, He Chang,
      Yunshan Ma, Yinwei Wei, Xiang Wang
    </>
  ),
  accepted: "",
  date: "Submitted to TIFS",
  remark: <></>,
  thumbnail: ToDA_figure,
  paper_link: "",
  code_link: "",
};
